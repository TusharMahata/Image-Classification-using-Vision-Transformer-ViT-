{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7d1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65712c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dictionary Hyperparameters  \n",
    "\n",
    "hp = {}\n",
    "hp['image_size'] = 200\n",
    "hp['num_channels'] = 3\n",
    "hp['patch_size'] = 25\n",
    "hp['num_patches'] = (hp['image_size']**2 / hp['patch_size']**2)\n",
    "hp['flat_patches_shape'] = (int(hp['num_patches']), hp['patch_size']*hp['patch_size']*hp['num_channels'])\n",
    "\n",
    "hp['batch_size'] = 32\n",
    "hp['lr'] = 1e-4\n",
    "hp['num_epochs'] = 500\n",
    "hp['num_classes'] = 5\n",
    "hp['class_name'] = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442c1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663714e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8813e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = shuffle(glob(os.path.join(path, '*', '*.jpg')))\n",
    "    print(int(len(images)))\n",
    "    split_size = int(len(images)* split)\n",
    "    #print(split_size)\n",
    "    train_x, test_x = train_test_split(images, test_size=0.1, random_state = 42)\n",
    "    print(int(len(train_x)), int(len(test_x)))\n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7811f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def process_image_label(path):\n",
    "    #path = path.decode('utf-8')\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp['image_size'], hp['image_size']))\n",
    "    image = image/255.0\n",
    "    #print(image.shape)\n",
    "    \n",
    "    # Preprocessing to patches \n",
    "    patch_shape = (hp['patch_size'], hp['patch_size'], hp['num_channels'])\n",
    "    patches = patchify(image, patch_shape, hp['patch_size'])\n",
    "    \n",
    "    #print(hp['flat_patches_shape'])\n",
    "    \n",
    "    patches_r = np.reshape(patches, hp['flat_patches_shape'])\n",
    "    patches_r = patches_r.astype(np.float32)\n",
    "\n",
    "    \n",
    "    patches = np.reshape(patches, (64, 25, 25, 3))\n",
    "    n = 8\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i in range(64):\n",
    "        #cv2.imwrite(f'files/{i}.png' ,patches[i])\n",
    "        ax = plt.subplot(n, n, i + 1)\n",
    "        patch_img = tf.reshape(patches[i], (hp['patch_size'], hp['patch_size'], 3))\n",
    "        plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    #patches_r = patches_r.astype(np.float32)\n",
    "    \n",
    "    # Label\n",
    "    print(path)\n",
    "    #path = str(path)\n",
    "    #class_name = []\n",
    "    \n",
    "    '''this is unbelieveable https://stackoverflow.com/questions/3167154/how-to-split-a-dos-path-into-its-components-in-python'''\n",
    "    \n",
    "    \n",
    "    class_name = path.split(os.sep)[-2]\n",
    "    #print(class_name)\n",
    "    class_idx = hp['class_name'].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "    print(class_name, class_idx)\n",
    "    \n",
    "    return patches_r, class_idx\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13e1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp['num_classes'])\n",
    "    \n",
    "    patches.set_shape(hp['flat_patches_shape'])\n",
    "    labels.set_shape(hp['num_classes'])\n",
    "    \n",
    "    return patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e70ccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "770d177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670\n",
      "3303 367\n",
      "C:\\Users\\DCL\\Image-Classification-using-Vision-Transformer-ViT-\\flower_photos\\daisy\\7191221492_610035de7c_m.jpg\n",
      "daisy 0\n",
      "ok\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFICAYAAADd1gwNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFOElEQVR4nO3bMaojQRAFwepF979yryXvk4wjpowIu41nJQUanXvvHQD+9O/tAQCbiSRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIHyePjzn/HLHI/XnoM37Nm+b2b1v87aZ3fs2b5vZv+/LJQkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECOfee98eAbCVSxIgiCRAEEmAIJIAQSQBwufpw3POL3c8Uj/Eb963edvM7n2bt83s3rd528z+fV8uSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAOPfe+/YIgK1ckgBBJAGCSAIEkQQIIgkQRBIgfJ4+POf8cscj9bXS5n2bt83s3rd528zufZu3zezf9+WSBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAHCuffet0cAbOWSBAgiCRBEEiCIJEAQSYAgkgDh8/ThOeeXOx6pr5U279u8bWb3vs3bZnbv27xtZv++L5ckQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCce+99ewTAVi5JgCCSAEEkAYJIAgSRBAifpw/POb/c8Uj9EL953+ZtM7v3bd42s3vf5m0z+/d9uSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAOHce+/bIwC2ckkCBJEECCIJEEQSIIgkQBBJgPB5+vCc88sdj9TXSpv3bd42s3vf5m0zu/dt3jazf9+XSxIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECOfee98eAbCVSxIgiCRAEEmAIJIAQSQBgkgChM/Th+ecX+54pL5W2rxv87aZ3fs2b5vZvW/ztpn9+75ckgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAcO699+0RAFu5JAGCSAIEkQQIIgkQRBIgfJ4+POf8cscj9UP85n2bt83s3rd528zufZu3zezf9+WSBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQRBIgiCRAEEmAIJIAQSQBgkgCBJEECCIJEEQSIIgkQBBJgCCSAEEkAYJIAgSRBAgiCRBEEiCIJEAQSYAgkgBBJAGCSAKEc++9b48A2MolCRBEEiCIJEAQSYAgkgBBJAGCSAIEkQQIIgkQ/gOgquZ0cF6KYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 400x400 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Seeding\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Directory for storing files\n",
    "    create_dir('files')\n",
    "    \n",
    "    # paths\n",
    "    dataset_path = r'C:\\Users\\DCL\\Image-Classification-using-Vision-Transformer-ViT-\\flower_photos'\n",
    "    model_path = os.path.join('files', 'model.h5')\n",
    "    csv_path = os.path.join('files', 'log.csv')\n",
    "    \n",
    "    train_x, test_x = load_data(dataset_path)\n",
    "    process_image_label(test_x[0])\n",
    "    \n",
    "    \n",
    "    train_ds = tf_dataset(train_x, batch=hp['batch_size'])\n",
    "    valid_ds = tf_dataset(test_x, batch=hp['batch_size'])\n",
    "    print('ok')\n",
    "    \n",
    "    #for x, y in train_ds:\n",
    "       # print(x.shape, y.shape)\n",
    "       # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf72db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
