{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7d1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify\n",
    "import implementation_ViT as vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65712c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dictionary Hyperparameters  \n",
    "\n",
    "hp = {}\n",
    "hp['image_size'] = 200\n",
    "hp['num_channels'] = 3\n",
    "hp['patch_size'] = 25\n",
    "hp['num_patches'] = 64\n",
    "hp['flat_patches_shape'] = (int(hp['num_patches']), hp['patch_size']*hp['patch_size']*hp['num_channels'])\n",
    "\n",
    "hp['batch_size'] = 32\n",
    "hp['lr'] = 1e-4\n",
    "hp['num_epochs'] = 500\n",
    "hp['num_classes'] = 5\n",
    "hp['class_name'] = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
    "\n",
    "\n",
    "hp[\"num_layers\"] = 12\n",
    "hp[\"hidden_dim\"] = 768\n",
    "hp[\"mlp_dim\"] = 3072\n",
    "hp[\"num_heads\"] = 12\n",
    "hp[\"dropout_rate\"] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "442c1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663714e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8813e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = shuffle(glob(os.path.join(path, '*', '*.jpg')))\n",
    "    print(int(len(images)))\n",
    "    split_size = int(len(images)* split)\n",
    "    #print(split_size)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
    "    print(int(len(train_x)), int(len(test_x)))\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7811f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def process_image_label(path):\n",
    "    #path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp['image_size'], hp['image_size']))\n",
    "    image = image/255.0\n",
    "    #print(image.shape)\n",
    "    \n",
    "    # Preprocessing to patches \n",
    "    patch_shape = (hp['patch_size'], hp['patch_size'], hp['num_channels'])\n",
    "    patches = patchify(image, patch_shape, hp['patch_size'])\n",
    "    \n",
    "    #print(hp['flat_patches_shape'])\n",
    "    \n",
    "    patches_r = np.reshape(patches, hp['flat_patches_shape'])\n",
    "    patches_r = patches_r.astype(np.float32)\n",
    "\n",
    "    \n",
    "    patches = np.reshape(patches, (64, 25, 25, 3))\n",
    "    n = 8\n",
    "    #plt.figure(figsize=(4, 4))\n",
    "    #for i in range(64):\n",
    "        #cv2.imwrite(f'files/{i}.png' ,patches[i])\n",
    "        #ax = plt.subplot(n, n, i + 1)\n",
    "        #patch_img = tf.reshape(patches[i], (hp['patch_size'], hp['patch_size'], 3))\n",
    "        #plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "        #plt.axis(\"off\")\n",
    "        \n",
    "    #patches_r = patches_r.astype(np.float32)\n",
    "    \n",
    "    # Label\n",
    "    #print(path)\n",
    "    #path = str(path)\n",
    "    #class_name = []\n",
    "    \n",
    "    '''this is unbelieveable https://stackoverflow.com/questions/3167154/how-to-split-a-dos-path-into-its-components-in-python'''\n",
    "    \n",
    "    \n",
    "    class_name = path.split(os.sep)[-2]\n",
    "    #print(class_name)\n",
    "    class_idx = hp['class_name'].index(class_name)\n",
    "    class_idx = np.array(class_idx, dtype=np.int32)\n",
    "    print(class_name, class_idx)\n",
    "    \n",
    "    return patches_r, class_idx\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e1fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp['num_classes'])\n",
    "    \n",
    "    patches.set_shape(hp['flat_patches_shape'])\n",
    "    labels.set_shape(hp['num_classes'])\n",
    "    \n",
    "    return patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e70ccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "770d177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670\n",
      "2936 367\n",
      "Train: 2936 - Valid: 367 - Test: 367\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"multi_head_attention_50\" (type MultiHeadAttention).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[768,12,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2]\n\nCall arguments received by layer \"multi_head_attention_50\" (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(None, 65, 768), dtype=float32)\n  • value=tf.Tensor(shape=(None, 65, 768), dtype=float32)\n  • key=None\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 29\u001b[0m\n\u001b[0;32m     23\u001b[0m valid_ds \u001b[38;5;241m=\u001b[39m tf_dataset(valid_x, batch\u001b[38;5;241m=\u001b[39mhp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#for x, y in train_ds:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m    \u001b[38;5;66;03m# print(x.shape, y.shape)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m    \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mvit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], clipvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m     33\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     36\u001b[0m     ModelCheckpoint(model_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     37\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     38\u001b[0m     CSVLogger(csv_path),\n\u001b[0;32m     39\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     40\u001b[0m ]\n",
      "File \u001b[1;32m~\\Image-Classification-using-Vision-Transformer-ViT-\\implementation_ViT.py:91\u001b[0m, in \u001b[0;36mViT\u001b[1;34m(cf)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Classification Head \u001b[39;00m\n\u001b[0;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m LayerNormalization()(x)\n",
      "File \u001b[1;32m~\\Image-Classification-using-Vision-Transformer-ViT-\\implementation_ViT.py:54\u001b[0m, in \u001b[0;36mtransformer_encoder\u001b[1;34m(x, cf)\u001b[0m\n\u001b[0;32m     52\u001b[0m skip_1 \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m LayerNormalization()(x)\n\u001b[1;32m---> 54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_heads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m Add()([x, skip_1])\n\u001b[0;32m     59\u001b[0m skip_2 \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\keras\\backend.py:2101\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2100\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2109\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2110\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2113\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2114\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"multi_head_attention_50\" (type MultiHeadAttention).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[768,12,768] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2]\n\nCall arguments received by layer \"multi_head_attention_50\" (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(None, 65, 768), dtype=float32)\n  • value=tf.Tensor(shape=(None, 65, 768), dtype=float32)\n  • key=None\n  • attention_mask=None\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "#from implementation_ViT import ViT\n",
    "#import implementation_ViT\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Seeding\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    # Directory for storing files\n",
    "    create_dir('files')\n",
    "    \n",
    "    # paths\n",
    "    dataset_path = r'C:\\Users\\DCL\\Image-Classification-using-Vision-Transformer-ViT-\\flower_photos'\n",
    "    model_path = os.path.join('files', 'model.h5')\n",
    "    csv_path = os.path.join('files', 'log.csv')\n",
    "    \n",
    "    train_x, valid_x, test_x = load_data(dataset_path)\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "\n",
    "    train_ds = tf_dataset(train_x, batch=hp[\"batch_size\"])\n",
    "    valid_ds = tf_dataset(valid_x, batch=hp[\"batch_size\"])\n",
    "    \n",
    "    #for x, y in train_ds:\n",
    "       # print(x.shape, y.shape)\n",
    "       # break\n",
    "    # Model\n",
    "    model = vit.ViT(hp)\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = tf.keras.optimizers.Adam(hp['lr'], clipvalue=1.0),\n",
    "        metrics = ['acc']\n",
    "    )\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n",
    "        CSVLogger(csv_path),\n",
    "        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=hp[\"num_epochs\"],\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[callbacks],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf72db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
